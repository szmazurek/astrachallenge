INFO:root:Starting training:
        Epochs:          5
        Batch size:      2
        Learning rate:   1e-05
        Training size:   0.8
        Validation size: 0.19999999999999996
        Validation freq: 2
        Checkpoints:     True
        Device:          cuda
        Mixed Precision: False
Epoch 1/5:   0%|          | 0/21 [00:00<?, ?img/s]/net/tscratch/people/plgjoanfr97/conda-envs/astra/lib/python3.12/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 128 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(

Epoch 1/5: 100%|██████████| 21/21 [00:23<00:00,  1.12s/img, loss (batch)=0.749]


Epoch 2/5: 100%|██████████| 21/21 [00:06<00:00,  3.36img/s, loss (batch)=0.435]
Traceback (most recent call last):
  File "/net/tscratch/people/plgjoanfr97/Files/GitRepos/AstraZeneca/Code/main.py", line 7, in <module>
    train_model(
  File "/net/tscratch/people/plgjoanfr97/Files/GitRepos/AstraZeneca/Code/train.py", line 135, in train_model
    val_score = evaluate(model, val_loader, device, amp, dice, epoch)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/net/tscratch/people/plgjoanfr97/conda-envs/astra/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/net/tscratch/people/plgjoanfr97/Files/GitRepos/AstraZeneca/Code/evaluate.py", line 49, in evaluate
    nib.save(nib.Nifti1Image(mask_pred[img].squeeze().numpy(), affine), f"./Validation-ep={epoch}/mask-pred_i={i}_b={img}.nii.gz")
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.