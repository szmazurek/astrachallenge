INFO:root:Starting training:
        Epochs:          5
        Batch size:      2
        Learning rate:   1e-05
        Training size:   0.8
        Validation size: 0.19999999999999996
        Validation freq: 2
        Checkpoints:     True
        Device:          cuda
        Mixed Precision: False
Epoch 1/5:   0%|          | 0/21 [00:00<?, ?img/s]/net/tscratch/people/plgjoanfr97/conda-envs/astra/lib/python3.12/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 128 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Epoch 1/5:   0%|          | 0/21 [00:20<?, ?img/s]
Traceback (most recent call last):
  File "/net/tscratch/people/plgjoanfr97/conda-envs/astra/lib/python3.12/site-packages/torchmetrics/metric.py", line 482, in wrapped_func
    update(*args, **kwargs)
  File "/net/tscratch/people/plgjoanfr97/conda-envs/astra/lib/python3.12/site-packages/torchmetrics/classification/dice.py", line 224, in update
    self.tp += tp
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!
The above exception was the direct cause of the following exception:
Traceback (most recent call last):
  File "/net/tscratch/people/plgjoanfr97/Files/GitRepos/AstraZeneca/Code/main.py", line 7, in <module>
    train_model(
  File "/net/tscratch/people/plgjoanfr97/Files/GitRepos/AstraZeneca/Code/train.py", line 101, in train_model
    loss += (1 - dice((F.sigmoid(masks_pred)>.5).int(), masks.int()))
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/net/tscratch/people/plgjoanfr97/conda-envs/astra/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/net/tscratch/people/plgjoanfr97/conda-envs/astra/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/net/tscratch/people/plgjoanfr97/conda-envs/astra/lib/python3.12/site-packages/torchmetrics/metric.py", line 311, in forward
    self._forward_cache = self._forward_reduce_state_update(*args, **kwargs)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/net/tscratch/people/plgjoanfr97/conda-envs/astra/lib/python3.12/site-packages/torchmetrics/metric.py", line 380, in _forward_reduce_state_update
    self.update(*args, **kwargs)
  File "/net/tscratch/people/plgjoanfr97/conda-envs/astra/lib/python3.12/site-packages/torchmetrics/metric.py", line 485, in wrapped_func
    raise RuntimeError(
RuntimeError: Encountered different devices in metric calculation (see stacktrace for details). This could be due to the metric class not being on the same device as input. Instead of `metric=Dice(...)` try to do `metric=Dice(...).to(device)` where device corresponds to the device of the input.